{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometrical Methods in Machine Learning\n",
    "\n",
    "## Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Intrinsic dimension estimation\n",
    "\n",
    "#### Correlation dimension\n",
    "\n",
    "Given a set $X_n = \\{x_1, \\dots, x_n\\}$ in a metric space, the _correlation dimension_ is defined\n",
    "\n",
    "$$C_n(r) = \\frac{2}{n(n -1)} \\sum_{i=1}^n \\sum_{j=i+1}^n \\mathbf{1}\\{\\|x_i - x_j\\| < r\\}.$$\n",
    "\n",
    "The correlation dimension is then estimated by plotting $\\log C_n(r)$ against $log(r)$ and estimating the slope of the linear part of the graph.\n",
    "\n",
    "You are asked to implement one of the following intrinsic dimension estimation methods:\n",
    "\n",
    "- correlation dimension, or\n",
    "- projection angle (Lecture 5 slides, pp. 30-33)\n",
    "\n",
    "Evaluate the obtained method and compare the estimates with global and local PCA and maximum likelihood estimation method for the airfoils, digits and faces from [Seminar 3](https://githib.com/oleg-kachan/GMML2020/seminar3) datasets and summarize the obtained results in a table. Conclude.\n",
    "\n",
    "Feel free to reuse the code from Seminars 1 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Manifold learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain `Extended Yale B` face dataset ([download](http://vision.ucsd.edu/extyaleb/CroppedYaleBZip/CroppedYale.zip)) which is comprised of 100x100 pixels images of 38 persons times 64 illumination conditions. Resize images to 32x32 pixels. You can do it using `Pillow` ([link](https://pillow.readthedocs.io/), tested) or any other image processing library of your choice.\n",
    "\n",
    "Estimate the intrinsic dimensionality with a method of your choice and perform dimensionality reduction to entrinsic dimension $\\hat{d}$ and dimensions 2 and/or 3 for visualization purposes using manifold learning methods of your choice.\n",
    "\n",
    "Compute NPR (neigborhood preservation ratio, see [Seminar 4](https://githib.com/oleg-kachan/GMML2020/seminar4)) of algorithms you have used for 2-3 different values of $d = \\{2$ and/or $3, \\hat{d} \\}$ and fixed number of nearest neighbors $k$. \n",
    "\n",
    "Explore the embedding space of size 2 and/or 3 for clusters and meaningful interpretations, comment the possible meaning of the new coordinates.\n",
    "\n",
    "Alternatively, you can perform this task on sklearn's `Olivetti faces` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra (+1 point)\n",
    "\n",
    "Implement the Laplacian eigenmaps algoritm. Test it on the Swiss roll dataset.\n",
    "\n",
    "### Laplacian eigenmaps\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "Given a dataset $\\mathbf{X} = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_n \\} \\in \\mathbb{R}^{D \\times n}$ in $D$-dimensional space,\n",
    "\n",
    "1. Estimate the neighborhood $\\mathcal{N}_i$ of each point, either with $\\varepsilon$-ball or take $k$-nearest neighbors graph (scikit-learn's `NearestNeighbors().fit(X).kneighbors_graph(mode=\"distance\")` will give you the matrix of pairwise distances $\\|\\mathbf{x}_i - \\mathbf{x}_j\\|$).\n",
    "\n",
    "2. Build the adjancency graph, with adjacency matrix $\\mathbf{A}$ s.t.\n",
    "\n",
    "$$a_{ij} = \\exp(-\\lambda \\| \\mathbf{x}_i - \\mathbf{x}_j \\|^2),~\\textrm{if}~\\mathbf{x}_j \\in \\mathcal{N}_i \\\\\n",
    "a_{ij} = 0,~\\textrm{otherwise},$$\n",
    "\n",
    "where $\\lambda$ is the scalar parameter, understood as a width of the kernel.\n",
    "\n",
    "3. Compute the graph Laplacian matrix $\\mathbf{L} = \\mathbf{D} - \\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, where $\\mathbf{D}$ s.t.\n",
    "\n",
    "$$d_{ii} = \\sum_i^n A_{ij}$$\n",
    "\n",
    "and solve the generalized eigenvalue problem (use SciPy's implementation of `scipy.linalg.eigh`, consider the matrix $\\mathbf{L}$ as the parameter $a$ and the matrix $\\mathbf{D}$ as the parameter $b$): \n",
    "\n",
    "$$\\mathbf{Lv} = \\lambda \\mathbf{Dv},\\\\\n",
    "\\mathbf{v} \\in \\mathbb{R}^n$$\n",
    "\n",
    "The first eigenvector $\\mathbf{v}_1$, corresponding to the _smallest_ eigenvalue will be zero vector $\\mathbf{0}$. Given $d$ eigenvectors $\\{ \\mathbf{v}_2, \\dots, \\mathbf{v}_{d+1} \\} \\in \\mathbb{R}^{n \\times d}$, corresponding to the $(d+1)$-th smallest eigenvalues, compute the $d$-dimensional emdedding $\\mathbf{Z}$:\n",
    "\n",
    "$$\\{\\mathbf{z}_1, \\dots, \\mathbf{z}_n \\} := \\{ \\mathbf{v}_2, \\dots, \\mathbf{v}_{d+1} \\}^T \\in \\mathbb{R}^{d \\times n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grading:\n",
    "8/10 points are awarded for completing all the tasks and giving proper answers to questions.  \n",
    "2/10 points are awarded for the quality of presentation, be sure to give explanations and comments to your solutions.  \n",
    "+1 extra point may be awarded for the extra work performed, be creative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
